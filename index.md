---
layout: archive
header:
  waves: true
author_profile: true
title: 
redirect_from: /about/
last_modified_at: "2025-10-31"
date: "2025-10-31"
---

<!-- name pronounciation example? [â–¶ï¸](https://upload.wikimedia.org/wikipedia/commons/c/c8/Example.ogg) -->

ðŸ‘‹ Hi, Iâ€™m Akash, an applied researcher/engineer with experience in speech, audio (at [Microsoft](https://www.microsoft.com/en-us/research/group/cognitive-services-research/speech/)), and most recently multi-modal document understanding and retrieval (at [Contextual AI](https://contextual.ai/)). Turns out this completes the trio of audio, vision & text AI multimodality. :)

Iâ€™m currently on a brief sabbatical, exploring ideas & tinkering as I work out whatâ€™s next. Currently exploring real-time, on-device neural audio in the context of music and voice.

# Work 

<details>
<summary>
<h2>Contextual AI</h2><h4>[2024-25]</h4>
<p>Wrangled millions of pages to land the first $ millions in enterprise contracts :) </p>
</summary>

<div markdown="1">
* 0â†’1: Built the core multimodal document understanding system (parsing, representation and ingestion) powering retrieval in the company's context platform for agents (i.e. RAG).
* Applied research: Vision models, VLM workflow/agent framework, document representation for agentic retrieval (demo below).
* Tech Lead / Manager: Mentored, managed, interviewed candidates, DRI with forward-deployed eng, marketing, PM.
* Links:
    * [Introducing the Document Parser for RAG](https://contextual.ai/blog/document-parser-for-rag/)
    * [Demo: llms.txt for Documents - Beyond Retrieval to Agentic Navigation](https://www.linkedin.com/feed/update/urn:li:activity:7346595035770929152?lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base_position_details%3BDHx4XhiTTqa5vQToJtWF3w%3D%3D)
</div>
</details>


<details>
<summary>
<h2>Microsoft</h2><h4>[2018-23]</h4>
<p>Fun fact: ~6M hours of monthly traffic equals 1 *year* of conversations transcribed per hour! </p>
</summary>

<div markdown="1">
* Shipped and optimized state of art models transcribing millions of hours of monthly conversations on Azure and Microsoft Teams APIs.
* Research engineering: data pipeline, distributed training framework, profiling, optimizing for inference in ONNX/C++
* Applied research: Scalability focused architecture design, data & training recipes, error analysis, evaluation metrics
* Was one of the few non-speech-PhD senior members on the team :)
* Links:
    * [Batch transcription - Azure AI Speech service](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/batch-transcription)
    * [US Patent US11563784B2: Caption assisted calling to maintain connection in challenging network conditions](https://patents.google.com/patent/US11563784B2/en)
</div>
</details>


# Misc 

- [2023] ðŸ¥ðŸ—£ï¸ Open source contribution to [whisper.cpp](https://twitter.com/ggerganov/status/1676271637572853771) (38k stars). [tinydiarize](https://github.com/akashmjn/tinyDiarize) is a lightweight prototype extending OpenAI's Whisper model for speaker diarization, runnable on Macbooks/iPhones.
- [2020] ðŸ‹ Co-founded [OrcaHello](https://ai4orcas.net/), a system for 24/7 monitoring of Southern Resident Killer Whales across many underwater hydrophones in the Pacific Northwest. It was awarded a [$30,000 AI for Earth Innovation Grant](https://wildlabs.net/funding-opportunity/ai-earth-innovation-grant-extended) in 2020 and has been operating live for >4 years - [listen here](http://orcahello.ai4orcas.net/Dashboard).

<!-- Don't hesitate to reach out at [akashmjn.1] [at] [gmail] [dot] [--] with any thoughts, collaborations, or opportunites! -->
